{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568eae0-cb4d-4e55-9790-201f2de7a373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import cartopy\n",
    "import dask\n",
    "from tqdm.autonotebook import tqdm  # Fancy progress bars for our loops!\n",
    "from dask.diagnostics import progress\n",
    "import intake\n",
    "import fsspec\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.rcParams['figure.figsize'] = 12, 6\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771a115-2ccc-4920-935c-eb14bad5abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\n",
    "col\n",
    "expts_full = ['historical','ssp126', 'ssp245', 'ssp370', 'ssp585', 'piControl']\n",
    "\n",
    "query = dict(\n",
    "    experiment_id=expts_full, # pick the `abrupt-4xCO2` and `piControl` forcing experiments\n",
    "    table_id='Amon',                            # choose to look at atmospheric variables (A) saved at monthly resolution (mon)\n",
    "    variable_id=['tas', 'pr','ua', 'va'],  # choose to look at near-surface air temperature (tas) as our variable\n",
    "    #level=[850]\n",
    "    member_id = 'r1i1p1f1',                     # arbitrarily pick one realization for each model (i.e. just one set of initial conditions)\n",
    ")\n",
    "\n",
    "col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "col_subset_var = [col_subset.search(variable_id=var_name) for var_name in query['variable_id']]\n",
    "col_subset.df[['source_id', 'experiment_id', 'variable_id', 'member_id']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dc876-433c-4de8-b63f-367377c55bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = col_subset_var[1].to_dataset_dict(\n",
    "    zarr_kwargs={\"consolidated\": True, \"decode_times\": True, \"use_cftime\": True}\n",
    ")\n",
    "ss = [key for key in dset_dict.keys() if 'piControl' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff51fd-72e3-4c01-84aa-f2b992694899",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in ss:\n",
    "    ds = dset_dict[s]\n",
    "    print('Starting time:',ds.time[0].values, '\\tEnding time:', ds.time[-1].values, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fee64-1397-44a1-9e44-538bf4d3ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all_bounds(ds):\n",
    "    drop_vars = [vname for vname in ds.coords\n",
    "                 if (('_bounds') in vname ) or ('_bnds') in vname]\n",
    "    return ds.drop(drop_vars)\n",
    "\n",
    "def open_dset(df):\n",
    "    #assert len(df) == 1\n",
    "    ds = xr.open_zarr(fsspec.get_mapper(df.zstore.values[0]), consolidated=True, decode_times=True, use_cftime=True)\n",
    "    if 'plev' in ds.coords:\n",
    "        for lev in ds.plev.values:\n",
    "            if int(lev)==85000:\n",
    "                ind = np.where(ds.plev.values==lev)\n",
    "                break\n",
    "        ds = ds.isel(plev=ind[0]).drop('plev')\n",
    "        #ds.drop('plev')\n",
    "    return drop_all_bounds(ds)\n",
    "\n",
    "def open_delayed(df):\n",
    "    return dask.delayed(open_dset)(df)\n",
    "\n",
    "from collections import defaultdict\n",
    "dsets = []\n",
    "for col_subset in col_subset_var :\n",
    "    dset = defaultdict(dict)\n",
    "\n",
    "    for group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n",
    "        dset[group[0]][group[1]] = open_delayed(df)\n",
    "    dsets.append(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81895b4-636d-449b-8b26-d51e19995abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with progress.ProgressBar():\n",
    "    dsets_ = dask.compute(dict(dsets[1]))[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf438a-3536-4170-b4c7-c0ac65e042ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymannkendall as mkt\n",
    "import esmvalcore.preprocessor as ecpr\n",
    "import dask.array as da\n",
    "import iris\n",
    "import numpy as np\n",
    "from cf_units import Unit\n",
    "import itertools\n",
    "def get_vname(ds):\n",
    "    #print(ds.variables)\n",
    "    for v_name in ds.variables.keys():\n",
    "        #print(v_name)\n",
    "        if v_name in ['pr', 'ua', 'va']:\n",
    "            return v_name\n",
    "    raise RuntimeError(\"Couldn't find a variable\")\n",
    "            \n",
    "def get_lat_name(ds):\n",
    "    for lat_name in ['lat', 'latitude']:\n",
    "        if lat_name in ds.coords:\n",
    "            return lat_name\n",
    "    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n",
    "    \n",
    "def get_lon_name(ds):\n",
    "    for lon_name in ['lon', 'longitude']:\n",
    "        if lon_name in ds.coords:\n",
    "            return lon_name\n",
    "    raise RuntimeError(\"Couldn't find a longitude coordinate\")\n",
    "\n",
    "def regrid(ds):\n",
    "    var_name = get_vname(ds)\n",
    "    #print(var_name)\n",
    "    ds = ds[var_name]\n",
    "    #ds_out = xe.util.grid_2d(-180.0, 180.0, 1.0, -90.0, 90.0, 1.0)\n",
    "    ds_out = xr.Dataset({\n",
    "        \"lat\": ([\"lat\"], np.arange(-90, 90, 1.0)),\n",
    "        \"lon\": ([\"lon\"], np.arange(-180, 180, 1.0)),\n",
    "    })\n",
    "    regridder = xe.Regridder(ds, ds_out, 'bilinear')\n",
    "    ds_reg = regridder(ds).to_dataset(name=var_name)\n",
    "    return ds_reg\n",
    "\n",
    "\n",
    "\n",
    "def jjas_mon_mean(ds):\n",
    "    var_name = get_vname(ds)\n",
    "    #print(ds.sel({'time':slice('2005', '2014')}))\n",
    "    ds_mon = ds.sel({'time':slice('2005', '2014')}).groupby('time.month').mean()\n",
    "    return ds_mon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc416377-ee0a-4b67-b1f5-599171efd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz.functoolz import juxt\n",
    "expt = expts_full[0]\n",
    "print(expt)\n",
    "#expt_da = xr.DataArray(expt, dims='experiment_id', name='experiment_id',\n",
    "#                       coords={'experiment_id': expt})\n",
    "\n",
    "dsets_aligned_list = []\n",
    "\n",
    "dsets_aligned = {}\n",
    "for k, v in tqdm(dsets_.items()):\n",
    "\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "\n",
    "    #for ds in expt_dsets:\n",
    "    #v.coords['year'] = v.time.dt.year\n",
    "\n",
    "    # workaround for\n",
    "    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "    dsets_time_mean = v[expt].pipe(regrid).pipe(jjas_mon_mean)\n",
    "\n",
    "    # align everything with the 4xCO2 experiment\n",
    "\n",
    "    dsets_aligned[k] = dsets_time_mean\n",
    "dsets_aligned_list.append(dsets_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744dbc5-06f5-4874-8b6f-0b65bf56081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with progress.ProgressBar():\n",
    "    dsets_algned_list_ = dask.compute(dsets_aligned_list[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4166207-23d1-4627-8630-18e6ce5ef3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = list(dsets_algned_list_.keys())\n",
    "#source_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n",
    "#                         coords={'source_id': source_ids})\n",
    "\n",
    "source_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n",
    "                         coords={'source_id': source_ids})\n",
    "big_ds_pr = xr.concat([ds.reset_coords(drop=True)\n",
    "                    for ds in dsets_algned_list_.values()],\n",
    "                    dim=source_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8699a-c300-4848-a0f2-d99d485c1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_ds_pr.to_netcdf('/home/jovyan/pangeo/data/precip_annual_cycle.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc38741-8655-4a6a-94fc-6ba4d8456227",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pr = xr.open_dataset('/home/jovyan/pangeo/data/precip_annual_cycle.nc')\n",
    "ds_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e89fd-0600-4c45-a3af-6f87d2bc02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esmvalcore.preprocessor as ecpr\n",
    "ucube = ds_pr.pr.rename({'lat':'latitude', 'lon':'longitude'}).to_iris()\n",
    "ulat = ucube.coord(\"latitude\")\n",
    "ulon = ucube.coord(\"longitude\")\n",
    "\n",
    "ulat.standard_name = \"latitude\"\n",
    "ulon.standard_name = \"longitude\"\n",
    "\n",
    "ucube.remove_coord(\"latitude\")\n",
    "ucube.add_dim_coord(ulat, 2)\n",
    "ucube.remove_coord(\"longitude\")\n",
    "ucube.add_dim_coord(ulon, 3)\n",
    "ucube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e82f-9fa3-4891-a87c-5e24eb666a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pr = xr.DataArray.from_iris(ecpr.mask_landsea(ucube, 'sea')).swap_dims({'dim_0':'source_id'}).sel(latitude=slice(5,28), longitude=slice(70,90))\n",
    "ds_pr_mean = ds_pr.mean(dim=('latitude', 'longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f856b2-d555-4009-b4ac-ce357f5c796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pr.isel(month=6, source_id=[0,1,2,3]).plot(col='source_id', col_wrap=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3018c1-9276-49a7-9c6c-0e00c581e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "mon = calendar.month_abbr[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206874c9-d35d-4692-9949-caa19d598583",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 5, dpi=300, figsize=(17, 18))\n",
    "fig.suptitle(\"Annual Precipitation cycle over India (2005-2014)\", x=0.5, y=0.92, fontsize=22, weight='bold')\n",
    "k = 0\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax = axs[i, j]\n",
    "        ss =  (ds_pr_mean.isel(source_id=k).values*86400).sum()\n",
    "        ax.bar(ds_pr_mean.isel(source_id=k).month, ds_pr_mean.isel(source_id=k).values*86400,)\n",
    "        ax.set_xticks(np.arange(1,13,3), mon[::3])\n",
    "        ax.set_ylabel('Precipitation (mm/day)')\n",
    "        ax.set_xlabel('Month')\n",
    "        ax.set_title(ds_pr_mean.isel(source_id=k).source_id.values)\n",
    "        ax.text(x=0.2, y= 0.9*ax.get_ylim()[1], s='p = '+str(round(ss,2))+'mm', c='b')\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "plt.savefig('/home/jovyan/pangeo/plot/pr_annual_cycle_allmodels.png', bbox_inches='tight', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3822e-e1f2-45ad-b713-9e972e8f296b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
